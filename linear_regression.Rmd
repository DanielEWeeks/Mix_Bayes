---
title: "Bayesian weighting of linear regression effect size"
author: "Daniel E. Weeks"
date: "`r format(Sys.time(), '%B %d, %Y, %R')`"
header-includes:
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
output:
  pdf_document:
    toc: true
    number_sections: true
    toc_depth: 3
  html_document:
    df_print: paged
    toc: true
    number_sections: true
    toc_depth: '3'
    code_folding: show
#   md_extensions: +raw_attribute
---

```{r,echo=FALSE,message=FALSE,warning=FALSE}
require(knitr)
# Set so that long lines in R will be wrapped:
# opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

```{r,echo=FALSE}
# In the event of a crash, save the working space on exit:
save_all <- function() {
  save.image("recover.RData")
}
options(error = save_all)
```

# Load Libraries

```{r load_libraries,message=FALSE}
library(tidyverse)
library(tidylog)
library(actuar)
library(LearnBayes)
library(broom)
```

# Input directory and files

```{r}
# Print the working directory
getwd()
```

The input files used in this analysis are ... and can be found in ...

# Example linear regression

    Estimate Std. Error t value
    0.288123   0.212662   1.355

    Chisq Df Pr(>Chisq)
    1.8356  1     0.1755

```{r}
x <- rnorm(3072)
y <- 0.2881*x + rnorm(3072,sd = 5)
m1 <- lm(y~x)
summary(m1)
a <- tidy(m1)
a$statistic[2]
2*(pt(a$statistic[2], df=3070-2-1, lower.tail = FALSE))
```

```{r}
pchisq(1.8356, 1, lower.tail = FALSE)
```

# FAB algorithm

## Set parameters

```{r}
# Illustration of FAB via simulated multiple t-tests from linear regression with selection of the largest t-statistic

L <- 5000 # number of regressions
N <- 3072
sN = sqrt(N) # sample size used to compute each T
sd <- 5
s0 <- 0.5^2 # prior variance for the distribution of the true difference (mu)
sd.s0 = sqrt(s0)
df.t = N - 2 - 1
m0 = 0 # prior for beta ~ N(m0, s0)
Cover = 0.95 # target coverage
trunc <- 0.99999
```

## Functions

```{r}
# Simulate Y ~ beta*X + e
# where e ~ N(0, sd)
sim.lm <- function(beta=0.2881, N=3072, sd=5) {
  x <- rnorm(N)
  y <- beta*x + rnorm(N,sd = sd)
  m1 <- lm(y~x)
  sd.y <- summary(m1)$sigma
  var.x <- var(x)
  var.y <- var(y)
  a <- tidy(m1)
  beta.hat <- a[2,]$estimate
  var.e <- var.y - (beta.hat^2)*var.x
  sd.ratio <- sqrt(var.x)/sqrt(var.e)
  unlist(c(a[2,c(4,3,2,5)],var.y=var.y, var.x=var.x, var.e=var.e,sd.ratio=sd.ratio))
}
#
# Density of a non-central t.
# This assumes z and nc are positive
fy.T <- function(z, df.t, nc) { dt(-z, df=df.t, ncp = (-nc)) }
#
# Compute the vector of posterior probabilities for the noncentrality parameter nc
Post.tab.T <- function(z, df.t, nc, b) {
  # Input
  # z: observed statistic
  # df.t: degrees of freedom
  # nc: non-centrality
  # b: prior probabilities
  nn = length(b)
  dn = fy.T(z, df.t, nc) %*% b
  pst = rep(0, nn)
  for(i in 1:nn) { pst[i] = fy.T(z, df.t, nc[i])*b[i] / dn }
  pst
}
```

## Discretized prior probabilities

```{r}
# Construct discretized distribution of prior probabilities
stp <- 0.002
px <- discretize(pnorm(x, mean=m0, sd=sd.s0),
                 method = "round",
                 from = qnorm(1-trunc, mean=m0, sd=sd.s0),
                 to = qnorm(trunc, mean=m0, sd=sd.s0), step = stp)
# Normalize to sum to one
px <- px/sum(px)
Num.Bins <- length(px)
cat(Num.Bins,"\n")
# Vector of effect sizes beta_i
fud <- seq(from=qnorm(1-trunc,mean=m0, sd=sd.s0), to=qnorm(trunc, mean=m0, sd=sd.s0)-stp, by=stp)
plot(fud,px, type="l", main="Prior distribution of the betas")
```

## Simulation

Here we simulate `r L` data sets with different true effect sizes, and then record the one with the largest positive t statistic.

```{r}
Beta <- rnorm(L, mean=m0, sd=sd.s0)
# Beta <- runif(L, min=0, max=1)
tmp.stat.mx <- c(-Inf, 1, 0)
jmx <- 1
# This assumes the simulated t statistic is positive
results <- data.frame(matrix(data=NA,nrow=L,ncol=8))
for(j in 1:L) {
    # Simulate a t statistic
    tmp.stat <- sim.lm(Beta[j], N, sd)
    if(tmp.stat[1] > tmp.stat.mx[1]) { tmp.stat.mx <- tmp.stat; jmx <- j }
    results[j,] <- tmp.stat
}
names(results) <- names(tmp.stat)
hist(results$statistic, breaks = 100, main="Histogram of simulated t statistics")
hist(results$estimate, breaks = 100, main="Histogram of simulated betas")
tmp.stat.mx
# Observed maximum t statistic
t.stat <- as.numeric(tmp.stat.mx[1])
sd.ratio <- as.numeric(tmp.stat.mx["sd.ratio"])
beta.hat <- as.numeric(tmp.stat.mx[3])
selected.beta <- Beta[jmx]
```

## Apply the FAB algorithm

The non-centrality equation for a t-test in a simple linear regression is:

$\lambda = \sqrt{N}(\beta - 0)\frac{\sigma_x}{\sigma_e}$

where

$\sigma_e = \sqrt{\sigma^2_y - \beta^2\sigma^2_x}$

```{r}
# Non-centrality parameters
# Two sample t-test: delta_i = sqrt(N)* mu_i/sigma.stat
# Simple linear regression: lambda_i = sqrt(N)*(beta -0)*sd.ratio
# where sd.ratio = sigma_x/sigma_e
fdn <-  (fud*sN)*sd.ratio
# Compute posterior probabilities of the non-centrality parameter
# pst = Pr(delta_i | p-value)
# Needs observed t statistic (t.stat), degrees of freedom (df.t),
# non-centrality parameters (fdn), prior probabilities (px)
pst <- Post.tab.T(t.stat, df.t, fdn, px) #Post.t(t.stat, df.t, fdn, px)
plot(fud,pst, type="l",main="Posterior probabilities")
abline(v=t.stat/sN/sd.ratio,col="red")
# E(delta | p-value) = Sum_i { delta_i * Pr(delta_i | p-value)}
# Posterior mean of the non-centrality
# E(delta | p-value) = sum(pst*fdn)
th.e <- (sum(pst*fdn)/sN)/sd.ratio # posterior mean, sigma estimated

# Matrix of means mu_i and their posterior probabilities
ddist <- cbind(fud, pst)
# Computes the highest probability interval for a discrete probability distribution
hpd.set <- discint(ddist, Cover)$set
mxbL <- hpd.set[1]
mxbU <- hpd.set[length(hpd.set)]
# Output:
# Lower Interval bound
# Posterior point estimate for beta
# Upper Interval Bound
# True mean difference
cat(mxbL, "<", th.e, "<", mxbU, "; True value:", selected.beta, "\n")
```

## Plot of the results

```{r}
txt <- paste0(round(mxbL,2), " < ", round(th.e,2), " < ", round(mxbU,2), "; True value: ", round(selected.beta,2),"; beta.hat: ",round(beta.hat,2))
plot(fud,pst, main=txt,type="l")
abline(v=th.e, col="red")
abline(v=selected.beta, col="green")
abline(v=beta.hat, col="brown")
abline(v=mxbL, col="blue",lty=2)
abline(v=mxbU, col="blue",lty=2)

```

# Application to our data

1. From the literature, assemble a list of all effect size estimates seen in prior studies.
2. Based on that list of effect size estimates, construct a discretized prior distribution of effect sizes.
    - Model this on what was done in the applied example in the paper.
    - See the author's 'Empirical_prior_plot.r' code.
3. Using that prior, apply the FAB algorithm.
    - Try simple linear regression as then degrees of freedom is well-defined. 
    - See discussion below.
4. Compare/contrast our frequentist results with our Bayesian results.
    - Expect that the 95% probability interval will be narrower.
    
Caveats:
Note that the linear mixed model software does not compute a p-value for the parameter-specific t-statistic because of statistical concerns. Given those concerns, this would be an approximation.

To apply the FAB algorithm, we need to know observed `t.stat` and the `sd.ratio`, which is $\sigma_x/\sigma_e$.

If these are our results

    Estimate Std. Error t value
    0.288123   0.212662   1.355

then we would set `t.stat <- 1.355`.

The `sd.ratio` can be computed from the data itself, as $\sigma_e = \sqrt{\sigma^2_y - \beta^2\sigma^2_x}$.

Would be interesting to see how similar/different the linear mixed model t-statistic is from a linear regression t-statistic where where we include only the fixed effects.  Probably better to use the results from the linear regression, as then the number of parameters in the model is clearly defined. Note that the degrees of freedom of the t statistic depends on the number of parameters `p` in the model: $df = N - p -1$.  




# Session Information

```{r}
sessionInfo()
```
